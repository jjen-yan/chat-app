{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjen-yan/chat-app/blob/master/CS97_Homework_1_Coding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the coding assignment for Homework 1. Some of the code blocks are already implemented, and you will not need to modify them (but you can modify them if you want to).\n",
        "\n",
        "What you **need to do** is to complete the code blocks marked with `# TODO`, and run the functions named `check_xxx` to check your implementation. If your code works as expected, you should see a printed message `Sanity check passed!` Otherwise, you will see an error message detailing what goes wrong.\n",
        "\n",
        "Some of those computations can be heavy and make take up to minutes, so please be patient. If you get anxious easily (like me), you can use `tqdm` to print a progress bar. Google it for its detailed usage."
      ],
      "metadata": {
        "id": "c6VUdpvQSRjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6.1 Loading word vectors\n",
        "\n",
        "Please download the `glove.6B.zip` file from https://nlp.stanford.edu/data/glove.6B.zip, unzip the zip file, and upload the `glove.6B.200d.txt` file to the `files` section of this notebook. This file contains GloVE embeddings learnt from 6B tokens. Each line represents a word, and each word is represented by a 200-dimension vector. For more information about GloVe algorithm, you can check https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "In Q6.1, your task is to understand the file structure and load the word embeddings."
      ],
      "metadata": {
        "id": "uwTmmhiARsiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can first read the file and understand its structure\n",
        "with open('./glove.6B.200d.txt') as f:\n",
        "    lines = f.readlines()\n",
        "for i, line in enumerate(lines[:5]):\n",
        "    print('--- Line', i)\n",
        "    print(line)\n",
        "print(\"Total length:\", len(lines))"
      ],
      "metadata": {
        "id": "i81tv2SXDpab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Coding Task 1: complete the `load_word_emb_dict` function to load the word embeddings into a dictionary.\n",
        "# The return value of the function should be a dictionary of 400k entries. Each entry should have a key of a word, and a value of a 200-dimensional torch tensor.\n",
        "# Note: remember to import the necessary packages like torch.\n",
        "def load_word_emb_dict():\n",
        "    # TODO\n",
        "\n",
        "word_emb_dict = load_word_emb_dict()"
      ],
      "metadata": {
        "id": "wS0RiCqvDopX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this code block to make sure your code is working as expected\n",
        "def check_load_word_emb_dict():\n",
        "    assert len(word_emb_dict) == 400000, f\"Dictionary size is incorrect: should be 400k, but is {len(word_emb_dict)}\"\n",
        "    for key, value in word_emb_dict.items():\n",
        "        assert value.shape == (200, ), f\"Vector shape is incorrect: should be 200 dimensions, but is {value.shape}\"\n",
        "    print(\"This task looks good!\")\n",
        "\n",
        "check_load_word_emb_dict()"
      ],
      "metadata": {
        "id": "jyNIvbyYRFIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6.2 Cosine similarity\n",
        "\n",
        "We can now define \"similar words\" as words with similar vectors. Recall from the lecture that we can measure the \"similarity\" between two words by measuring the cosine similarity between their word embeddings. For two vectors, `u` and `v`, their cosine similarity is defined as diving their dot product by the L2-norm (i.e. length) of the two vectors. Formally, it is computed as:\n",
        "\n",
        "$$\\text{cosine_similarity}(u, v) = \\frac{u~\\cdot~v}{||u||_2 ~\\cdot~||v||_2}$$\n",
        "\n",
        "$$u\\cdot v = \\sum_{i=1}^d u_i \\cdot v_i, ~||u||_2 = \\sqrt{\\sum_{i=1}^d u_i^2}$$\n",
        "\n",
        "where `d` is the dimension of `u` and `v`.\n",
        "\n",
        "A good way to understand this formula is that the numerator cares about whether the two vectors are correlated. If they tend to have similar values, the numerator tends to be big. The denominator can be thought of as a normalization factor: if all the values of `u` and `v` are really large, for example, dividing by the square root of their sum-of-squares prevents the whole thing from getting arbitrarily large.\n",
        "\n",
        "PyTorch has their official implementation of cosine similarity here https://pytorch.org/docs/stable/generated/torch.nn.functional.cosine_similarity.html. However, in this problem, you **cannot** call PyTorch, or any other libraries' own implementation of cosine similarity. You should implement cosine similarity based on its definition. You **can** call PyTorch, or any other libraries implementation of dot product and L2-norm, though you are encouraged to implement them by yourself.\n",
        "\n",
        "For reference, you may use the following links helpful:\n",
        "\n",
        "1. PyTorch's dot product: https://pytorch.org/docs/stable/generated/torch.dot.html\n",
        "2. PyTorch's vector norm: https://pytorch.org/docs/stable/generated/torch.linalg.vector_norm.html"
      ],
      "metadata": {
        "id": "L4EPqS_stL78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Coding Task 2: complete the `cosine_similarity` function to compute the cosine similarity between vectors u and v.\n",
        "# Both u and v are torch tensors of 200 dimensions.\n",
        "def cosine_similarity(u, v):\n",
        "    # TODO"
      ],
      "metadata": {
        "id": "rqp4RfCnRFLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this code block to make sure your code is working as expected\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def check_cosine_similarity():\n",
        "    words = list(word_emb_dict.keys())\n",
        "    for _ in range(10):  # test 10 random pairs of words\n",
        "        w1 = random.choice(words)\n",
        "        w2 = random.choice(words)\n",
        "        v1 = word_emb_dict[w1]\n",
        "        v2 = word_emb_dict[w2]\n",
        "        cosine_official = torch.nn.functional.cosine_similarity(v1, v2, dim=0)\n",
        "        cosine_ours = cosine_similarity(v1, v2)\n",
        "        assert (cosine_official - cosine_ours).abs() < 1e-6, f\"Your implementation deviates from PyTorch official implementation for these two words: {w1}, {w2}\"\n",
        "    print(\"This task looks good!\")\n",
        "\n",
        "check_cosine_similarity()"
      ],
      "metadata": {
        "id": "ZkCtveVaRFNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6.3 Nearest Neighbors\n",
        "\n",
        "With the cosine similarity function, we could find the words that are most similar to any given word, which are called the **nearest neighbors** of the given word.\n",
        "\n",
        "For reference, you may find this link helpful: https://pytorch.org/docs/stable/generated/torch.topk.html"
      ],
      "metadata": {
        "id": "R5fOp3e7y58s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Coding Task 3: complete the `find_nearest_words` function using your own implemented `cosine_similarity` function.\n",
        "# For each given word `word`, the return value should be a list of `n` words that are most similar to `word` (excluding `word` itself).\n",
        "# The list should be sorted from the most similar word to the n-th most similar word.\n",
        "def find_nearest_words(word, n):\n",
        "    # TODO"
      ],
      "metadata": {
        "id": "gCfjwrjY2NAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this code block to make sure your code is working as expected\n",
        "def check_find_nearest_words():\n",
        "    words = find_nearest_words('orange', 10)\n",
        "    assert isinstance(words, list), \"Incorrect output format: the output should be a list\"\n",
        "    assert len(words) == 10, \"Incorrect output format: the output should be of length 10\"\n",
        "    assert words[0] == 'yellow', f\"The implementation looks wrong: the most similar word should be yellow, but instead your result is {words[0]}\"\n",
        "    print(\"This task looks good!\")\n",
        "\n",
        "check_find_nearest_words()"
      ],
      "metadata": {
        "id": "Yi63ZzRL4WGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6.4 Writing Problem I\n",
        "\n",
        "Using your own implemented `cosine_similarity` function, compute the similarity between each pair of words out of the following eight words:\n",
        "\n",
        "    happy, glad, red, blue, purple, physics, chemistry, biology\n",
        "\n",
        "Describe your observations."
      ],
      "metadata": {
        "id": "WHBFHKL46NMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "ZlQK-wl65LAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6.5 Writing Problem II\n",
        "\n",
        "Using your own implemented `cosine_similarity` function, compare the cosine similarity between the following word pairs:\n",
        "\n",
        "* police - man\n",
        "* police - woman\n",
        "* terrorism - muslim\n",
        "* terrorism - christian\n",
        "\n",
        "Describe your observations, and provide some intuition about why this would happen. What insights does it give us for designing future NLP systems?"
      ],
      "metadata": {
        "id": "HxcgZa_o-uy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "eZDGIWgv-5jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6.6 Writing Problem III\n",
        "\n",
        "Use your `find_nearest_words` function, find out what are the most similar words to each of the following words:\n",
        "\n",
        "    boston, fish, communist, professor, microsoft, right, park, date, train, ship\n",
        "\n",
        "What are some examples that work \"well\"? Please provide three examples."
      ],
      "metadata": {
        "id": "2LsjlrG2_pjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "KhGADJSO_un3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}